I am a PhD student in the Computer Science Department at Carnegie Mellon University, advised by [Matt Fredrikson](https://www.cs.cmu.edu/~mfredrik/).

My current research deals with privacy and fairness issues in machine learning. I am also broadly interested in security and privacy.

## Publications
<table>
  <tr>
    <td>
      <a href="https://arxiv.org/abs/1808.08619">arXiv</a>
    </td>
    <td>
      <b>Avoiding Disparity Amplification under Different Worldviews</b><br>
      Samuel Yeom and Michael Carl Tschantz<br>
      In submission
    </td>
  </tr>
  <tr>
    <td>
      <a href="https://www.ijcai.org/Proceedings/2020/61">paper</a>
      <a href="https://arxiv.org/abs/2002.07738">arXiv</a>
    </td>
    <td>
      <b>Individual Fairness Revisited: Transferring Techniques from Adversarial Robustness</b><br>
      Samuel Yeom and Matt Fredrikson<br>
      <i>International Joint Conference on Artificial Intelligence</i>, 2020<br><br>
      Note: The conference version has a minor error in the proof of Theorem 3. This is fixed in the arXiv version.
    </td>
  </tr>
  <tr>
    <td>
      <a href="http://proceedings.mlr.press/v108/tan20a.html">paper</a>
    </td>
    <td>
      <b>Learning Fair Representations for Kernel Models</b><br>
      Zilong Tan, Samuel Yeom, Matt Fredrikson, and Ameet Talwalkar<br>
      <i>Conference on Artificial Intelligence and Statistics</i>, 2020
    </td>
  </tr>
  <tr>
    <td>
      <a href="https://dl.acm.org/doi/abs/10.1145/3351095.3372845">paper</a><br>
      <a href="https://github.com/samuel-yeom/fliptest">code</a>
    </td>
    <td>
      <b>FlipTest: Fairness Testing via Optimal Transport</b><br>
      Emily Black<sup>&ast;</sup>, Samuel Yeom<sup>&ast;</sup>, and Matt Fredrikson<br>
      <i>ACM Conference on Fairness, Accountability, and Transparency</i>, 2020
    </td>
  </tr>
  <tr>
    <td>
      <a href="https://content.iospress.com/articles/journal-of-computer-security/jcs191362">paper</a>
    </td>
    <td>
      <b>Overfitting, Robustness, and Malicious Algorithms: A Study of Potential Causes of Privacy Risk in Machine Learning</b><br>
      Samuel Yeom, Irene Giacomelli, Alan Menaged, Matt Fredrikson, and Somesh Jha<br>
      <i>Journal of Computer Security</i>, 2020
    </td>
  </tr>
  <tr>
    <td>
      <a href="https://papers.nips.cc/paper/7708-hunting-for-discriminatory-proxies-in-linear-regression-models">paper</a><br>
      <a href="https://github.com/samuel-yeom/linreg-proxy">code</a>
    </td>
    <td>
      <b>Hunting for Discriminatory Proxies in Linear Regression Models</b><br>
      Samuel Yeom, Anupam Datta, and Matt Fredrikson<br>
      <i>Advances in Neural Information Processing Systems</i>, 2018
    </td>
  </tr>
  <tr>
    <td>
      <a href="https://ieeexplore.ieee.org/document/8429311">paper</a><br>
      <a href="https://github.com/samuel-yeom/ml-privacy-csf18">code</a>
    </td>
    <td>
      <b>Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting</b><br>
      Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha<br>
      <b>Distinguished Paper</b> at the <i>IEEE Computer Security Foundations Symposium</i>, 2018
    </td>
  </tr>
</table>

<sup>*</sup> Equal contribution

## Miscellaneous
I am an Officer in [Puzzle Hunt CMU](https://puzzlehunt.club.cc.cmu.edu/), which is a student club that runs a puzzle hunt every semester.
(If you don't know what a puzzle hunt is, click the link to find out!)
